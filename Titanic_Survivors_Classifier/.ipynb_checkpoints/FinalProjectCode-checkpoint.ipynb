{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heath Thomspon\n",
    "Computer Science\n",
    "CS 477 - Spring 2021\n",
    "Final Project - Predicting Survivors on the Titanic Using Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import matplotlib.pyplot as matplot\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, precision_recall_curve\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "train.shape = (891, 12)\n",
    "test.shape = (418, 11)\n",
    "\n",
    "Remove Unnecessary Data\n",
    "Looking at the data using train.head(), I saw several features that should have zero weight on whether or not a passenger died. These features include PassengerId, Name, Ticket, Cabin. I removed these from the datasets.\n",
    "\n",
    "Convert Qualitative to Quantitative\n",
    "I need to also convert any data that is qualatative into quantitative data. I do so using pandas.get_dummies() which turns all qualatative data into binary data types. If the column has more than two types of input. ex.dog,cat,bird, it will create a colums for each. If yes, then 1 else 0.\n",
    "\n",
    "Fill Missing Values\n",
    "Lastly, I need to make sure my train and test data contain no null/NA values as I don't know how these will affect the logistic regression model. Using isnull(), I found that hundreds of cells of missing data under the age column. Although it would be easier to just delete this column, it's necessary to keep it because age has an important role in whether or not someone lives. I could also remove the samples from the data that have null age values, but because there are so many that are null (train: 177 test:86), roughly 25% or all samples, that may not be the best option. This leaves me with the decision to replace null cells with a value. I chose to use the mean of the column to fill these null cells. I have read about bfill and ffill and may later on choose those if I believe they will make the model more accurate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-92-d7bb0d93745c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;31m#fill null cells with the mean age in both train and test sets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Age'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Age'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mskipna\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m \u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Age'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Age'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mskipna\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;31m#after preprocessing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "#paths to test and train data sets provided by https://www.kaggle.com/c/titanic/data\n",
    "trainloc = \"C:\\\\Users\\\\Heath\\\\OneDrive\\\\Documents\\\\CS\\\\CS477\\\\FinalProject-TitanicSurvivors\\\\train.csv\"\n",
    "testloc = \"C:\\\\Users\\\\Heath\\\\OneDrive\\\\Documents\\\\CS\\\\CS477\\\\FinalProject-TitanicSurvivors\\\\test.csv\"\n",
    "\n",
    "\n",
    "#load datasets inot test and train variables\n",
    "trainraw = pandas.read_csv(trainloc)\n",
    "train = trainraw.copy()\n",
    "#testraw = pandas.read_csv(testloc)\n",
    "#test = testraw.copy()\n",
    "#train.head()    #used to get a quick look at data\n",
    "    \n",
    "#print(\"Train: \", train.shape)\n",
    "#print(\"Test: \", test.shape)\n",
    "\n",
    "#remove unnecessary data from training set\n",
    "train.drop('PassengerId', axis=1, inplace=True)\n",
    "train.drop('Name', axis=1, inplace=True)\n",
    "train.drop('Ticket', axis=1, inplace=True)\n",
    "train.drop('Cabin', axis=1, inplace=True)\n",
    "#remove unnecessary data from test set, not used in proect, do not have target data for test\n",
    "#test.drop('PassengerId', axis=1, inplace=True)\n",
    "#test.drop('Name', axis=1, inplace=True)\n",
    "#test.drop('Ticket', axis=1, inplace=True)\n",
    "#test.drop('Cabin', axis=1, inplace=True)\n",
    "\n",
    "#covert Pclass, Sex, and Embarked columns from categorical data to binary data using one hot encoding\n",
    "train = pandas.get_dummies(train, columns=['Pclass', 'Sex', 'Embarked'])\n",
    "#test = pandas.get_dummies(test, columns=['Pclass', 'Sex', 'Embarked'])\n",
    "\n",
    "#check for missing values in datasets\n",
    "#train.isnull().sum()            #177 null age values\n",
    "#test.isnull().sum()             #86 null age values\n",
    "\n",
    "#fill null cells with the mean age in both train and test sets\n",
    "train['Age'].fillna(train['Age'].median(skipna=True), inplace=True)\n",
    "test['Age'].fillna(test['Age'].median(skipna=True), inplace=True)\n",
    "\n",
    "#after preprocessing\n",
    "print(\"Train: \", train.shape)\n",
    "print(\"Test: \", test.shape)\n",
    "print(train.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model\n",
    "Here we train a logistic regression model before any we apply any optimization techniques.\n",
    "After training the model, I need to make sure to scale the features to prevent any features from being given too much weight in the model. I will do this using standardization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (712, 12) \n",
      "Test: (179, 12)\n",
      "Train: (712,) \n",
      "Test: (179,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = train.drop('Survived', axis=1)\n",
    "y = train['Survived']\n",
    "\n",
    "#split data into training anf validation sets\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size = 0.20, random_state = 0)\n",
    "print(f\"Train: {X_train.shape} \\nTest: {X_test.shape}\")\n",
    "print(f\"Train: {y_train.shape} \\nTest: {y_test.shape}\")\n",
    "\n",
    "#standardizing features in our training, validation, and test sets\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "#print(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "#test = scaler.transform(test) used for kaggle competition, but not project as I do not have target data for this test set\n",
    "\n",
    "#training to logistic regression model\n",
    "model1 = LogisticRegression(random_state=0)\n",
    "model1.fit(X_train, y_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model\n",
    "This is where we test the model to study performance/accuracy before we optimize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.8019662921348315\n",
      "Testing Accuracy:  0.8044692737430168\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Accuracy: \", model1.score(X_train, y_train))\n",
    "print(\"Testing Accuracy: \", model1.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning the model using GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters:  {'C': 0.1, 'penalty': 'l2'}\n",
      "best score:  0.8007092198581559\n"
     ]
    }
   ],
   "source": [
    "model2 = LogisticRegression(random_state=0, solver='liblinear')\n",
    "params = {  \n",
    "            'penalty' : ['l1', 'l2'],\n",
    "            'C' : [0.01, 0.1, 1, 10, 100, 1000] }\n",
    "\n",
    "model2 = GridSearchCV(estimator=model2, cv=15, param_grid=params, scoring='accuracy')\n",
    "\n",
    "model2.fit(X_train, y_train)\n",
    "print(\"best parameters: \", classifier.best_params_)\n",
    "print(\"best score: \", classifier.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8044692737430168\n",
      "Recall:  0.7391304347826086\n",
      "Precision:  0.75\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = model2.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "print(\"Accuracy: \", (cm[0][0]+cm[1][1])/y_test.shape[0])\n",
    "print(\"Recall: \", (cm[1][1]/(cm[1][0]+cm[1][1])))\n",
    "print(\"Precision: \", (cm[1][1]/(cm[1][1]+cm[0][1])))\n",
    "\n",
    "y_score = model2.decision_function(X_test)\n",
    "fpr, tpr, thresh = roc_curve(y_test, y_score)\n",
    "auc = auc(fpr,tpr)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#binary cross entropy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
